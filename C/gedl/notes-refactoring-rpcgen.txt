RPCGenerator
 -- located in build/capo/C/gedl
 -- inputs: 
    partitioned application code including name of main program
    GEDL: JSON with function signatures of functions to be wrapped 
          in cross-domain RPC plus information about which parameters
          are inputs/outputs/both, and size of arrays etc.
          currently function arguments must be primitive types or 
          fixed size arrays of primitive types
    base value for mux, sec, and typ assignments for this progam
    list of enclaves and their levels
    URI to use for HAL configuration

    UPenn also needs the collated CLE-JSON file as input
    (two reasons: UPenn has additional parameters in the CLE used for
     reliable RPC generation; limitation of our old version applied 
     oneway or bidirection uniformly to all functions and did not 
     consider the respect CLE annotation for each individual function.)

    (note: separately an IDL is generated from the GEDL for the
     cross-domain data types, and from the IDL serialization codecs
     are genreated for use with HAL API. RPCGenerator must be consistent
     with the sequence and naming conventions used in the IDL generator 
     and codec generator. DFDL is also generated from the IDL separately.)

 -- outputs:
    1. C and header files for CLE-annotated RPC code for each partition 
       (includes ithe RPC wrapper and peer call handler)
    2. Modifications to the partitioned application code (heuristic, brittle)
     - add HAL init and RPC headers to main program
     - replace cross domain calls foo() with _rpc_foo()
     - on side without the main, create a main program and a handler loop
    Additionally proto-HAL config is generated (lacks device specific config)

    (note: separate script takes the proto-HAL config plus a user-provided 
     device.json to generate complete HAL config for each side)
  
  -- modes (instantiated using C preprocessor macros for conditional compilation):

     1. singlethreaded vs. multithreaded (default)
      - latter provides one RPC listener/subscriber thread per XD function 
        RPC protocol is simply get request, run function, send response
      - former special case for completely singlethreaded programs;
        extra message exchange is required in the single listener thread
          callee (litener thread) waits for nextrpc message
          caller first sends nextrpc type
          callee (litener thread) sends okay
          callee (litener thread) waits for message specified in prev nextrpc 
          caller sends actual request
          callee gets requests, runs function, send response
          continue looping
     (note: here because Columbia requested this for ease of analysis, sees 
      little usage currently, but support for single-threaded apps is nice to
      have)

      2. legacy vs. my_xdc
      - original design was based on one persistent listener that handled
        cross-domain messages -- this thread opened a zeromq socket once
        and reused it for the life of the program: 0MQ sockets are not
        thread-safe (0MQ contexts are thread-safe)
      - original design did not work in the case of secdesk, which used
        a web application framework that assigned each HTTP request to
        an arbitrary therad, which had to XD calls. So we needed an 
        alternative where the socket is opened and closed just in time
        within the thread in question -- less efficent, but thread-safe

      (note: legacy put the code in hal/api/xdcomms.[ch], however my_xdc 
       includes a complete alternative thread-safe implementation if
       xdcomms within the genreated <foo>_rpc[.ch].  Somewhat asymmetric.
       xdcomms.[ch] is also used by the Java toolchain. Perhaps the best
       thing is to move my_xdc with conditional compilation into xdcomms.[ch]
       If this done then RPCGenerator will be simplified. When building the
       XDCOMMS library build two versions -- one for legacy one for myxdc)

      3. with and without UPenn reliability
     
      - to be integrated (UPenn code is in a separate branch)
      - Phase 1 RPC genreator is not tolerant to delay/loss -- upon
        loss, the RPC call will hang forever -- we added a variant of
        xdc_blocking_recv which allowed timeouts. UPenn uses this and
        modified each generated RPC wrapper and handler to include 
        retries. CLE annotations specify timeout value and number of
        retries.
      - changes affect the CLE schema used by CLE preprocessor (additional
        fields in CLE)
      - additional code in the RPC generator to implement timeout and retry
      - annotated example application which exercises UPenn features
      - additional input to RPC generator, namely CLE json
      - change in invocation syntax in .vscode/build scripts to include 
        pass the additional input param

